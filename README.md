# MLOps와 Docker를 활용한 AI 모델 자동화 파이프라인 구축 기술 세미나

[![MLOps Workflow](MLOps_workflow.png)](MLOps_workflow.png)

## 🚀 세미나 개요

본 세미나에서는 AI 모델이 어떻게 스스로 학습하고, 성능을 개선하며, 서비스에 자동으로 배포되는지를 시연하는 **엔드투엔드(End-to-End) MLOps 파이프라인** 구축 사례를 소개합니다.

**Docker**를 활용하여 각 구성 요소를 마이크로서비스로 분리하고, **MLflow**를 중심으로 모델의 전체 생명주기를 관리하는 과정을 비트코인 가격 예측 모델 예제를 통해 직접 보여드립니다. 저희의 목표는 복잡한 MLOps 개념을 누구나 쉽게 이해하고 실제 프로젝트에 적용할 아이디어를 얻어 가시도록 돕는 것입니다.

---

## 🎯 대상 청중

*   **AI/ML 모델을 개발하고 운영**하는 데 관심 있는 개발자 및 학생
*   **MLOps 파이프라인**의 실제 구현 사례가 궁금하신 분
*   **Docker와 MLflow**를 활용한 모델 배포 및 관리 자동화에 대해 배우고 싶으신 분
*   AI 엔지니어, 데이터 사이언티스트, 백엔드 개발자 등 AI 서비스화에 관련된 모든 분

---

## 📚 학습 목표

이 세미나를 통해 다음을 배우고 이해하게 됩니다.

*   **MLOps의 핵심 개념**: 모델 학습, 평가, 배포, 모니터링의 자동화된 흐름을 이해합니다.
*   **컨테이너 기반 아키텍처**: Docker를 사용하여 MLOps 구성 요소(추론, 학습, DB 등)를 독립적인 서비스로 구축하고 관리하는 방법을 배웁니다.
*   **MLflow 활용법**: 실험 추적(Tracking), 모델 등록(Registry), 모델 서빙(Serving) 등 MLflow의 핵심 기능을 실제 코드와 함께 학습합니다.
*   **자동화된 모델 재학습 및 배포**: 새로운 데이터가 쌓였을 때, 자동으로 모델을 재학습하고 기존 모델과 성능을 비교하여 더 나은 모델을 서비스에 배포하는 파이프라인을 이해합니다.
*   **엔드투엔드 파이프라인 구축**: 데이터 수집부터 모델 추론 API 제공까지, 전체 MLOps 파이프라인이 어떻게 유기적으로 동작하는지 파악합니다.

---

## 📜 세미나 아젠다

| 시간 (분) | 세션 | 내용 |
| :--- | :--- | :--- |
| 10 | **오프닝** | 세미나 소개 및 MLOps의 중요성 |
| 20 | **1. 아키텍처 및 시나리오 소개** | - 전체 시스템 아키텍처 분석<br>- '자동으로 똑똑해지는 AI 모델' 시나리오 설명 |
| 25 | **2. 핵심 컴포넌트 분석** | - **Docker**: 각 서버를 컨테이너로 격리하는 이유<br>- **MLflow**: 모델 생명주기 관리의 핵심<br>- **FastAPI/학습 서버**: 역할과 책임 분리 |
| 30 | **3. 라이브 데모 시연** | - **데이터 수집**: 실시간 데이터 DB 적재<br>- **자동 학습**: 새 데이터로 '챌린저' 모델 학습 및 MLflow 기록<br>- **성능 비교**: '챔피언' vs '챌린저' 모델 성능 검증<br>- **자동 배포**: '챔피언' 교체 및 API 서버에 자동 반영 |
| 15 | **4. 코드 리뷰 및 Q&A** | - 주요 코드 분석 및 핵심 로직 설명<br>- 자유로운 질의응답 |
| 10 | **클로징** | - 요약 및 향후 발전 방향<br>- 참고 자료 공유 |

---

## 🛠️ 사전 요구 사항

*   **기본 지식**: Python, REST API, 머신러닝 모델링에 대한 기본적인 이해가 있으면 좋습니다.
*   **Docker**: Docker 및 Docker Compose에 대한 기본 개념을 알고 오시면 더욱 좋습니다. (필수 아님)
*   **로컬 환경에서 직접 실행해 보려면**:
    *   [Docker Desktop](https://www.docker.com/products/docker-desktop/) 설치
    *   `git` 설치

---

## 👨‍🏫 발표자 소개

본 세미나는 **우리 FISA 5기 AI 엔지니어링 과정**의 다음 팀원들이 진행합니다.

*   **발표자 1** (담당: 아키텍처, MLflow)
*   **발표자 2** (담당: 학습/추론 서버, 데모 시연)

---

## 🔗 관련 자료 및 소스 코드

*   **GitHub Repository**: [https://github.com/your-repo/tech-semina](https://github.com/your-repo/tech-semina)
    *   `MLOps_scenario.md`: 세미나의 전체 시나리오를 상세히 설명합니다.
    *   `about_diagram.md`: 시스템 아키텍처 다이어그램을 분석하고 설명합니다.
    *   `role_of_each_server.md`: 각 서버의 역할과 책임을 Q&A 형식으로 정리했습니다.
*   **주요 디렉토리 가이드**:
    *   `/mlflow-client`: 모델 학습, 평가, 백테스팅을 수행하는 **학습 서버**의 모든 코드가 담겨있습니다.
    *   `/mlops_inference_server`: 예측 요청을 처리하는 **FastAPI 추론 서버**의 코드입니다.
    *   `/mlops-db-ingestion`: 초기 데이터를 DB에 적재하는 스크립트입니다.
    *   `/visualizer-server`: 예측 결과를 시각화하는 **대시보드 앱** 코드입니다.

---

## 🚀 시작하기 (로컬에서 실행)

본 프로젝트를 로컬 환경에서 직접 실행하고 MLOps 파이프라인을 체험해볼 수 있습니다.

1.  **레포지토리 클론**
    ```bash
    git clone https://github.com/your-repo/tech-semina.git
    cd tech-semina
    ```

2.  **환경 변수 설정**
    `mlops-db-ingestion` 디렉토리의 `.env.sample` 파일을 `.env`로 복사하고, 필요한 환경 변수를 설정합니다. (세미나에서 안내)

3.  **Docker Compose 실행**
    ```bash
    docker compose up -d --build
    ```

4.  **상태 확인**
    ```bash
    docker compose ps
    ```
    모든 컨테이너가 `Up` 상태인지 확인합니다. 이제 각 서버의 엔드포인트로 접속하여 파이프라인을 테스트할 수 있습니다.

---

## 📞 문의하기

세미나 내용이나 프로젝트에 대해 궁금한 점이 있으시면 언제든지 아래 방법으로 문의해주세요.

*   **GitHub Issues**: 레포지토리의 [Issues](https://github.com/your-repo/tech-semina/issues) 탭에 질문을 남겨주세요.
*   **이메일**: `contact@example.com`
