from typing import Optional

import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler

import torch
from torch import nn
from torch.optim import Optimizer
from torch.utils.data import TensorDataset, DataLoader

import mlflow


class LSTM(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, num_layers: int, output_size: int):
        super(LSTM, self).__init__()
        # Initialize parameters
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Define LSTM layer and fully connected layer
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x) -> torch.Tensor:
        # Initialize hidden state and cell state with zeros
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)
        
        # Forward pass through LSTM
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])  # take the last time step
        return out

class LstmWithScaler(mlflow.pyfunc.PythonModel):
    def __init__(self,
                 model: LSTM,
                 x_scaler: StandardScaler,
                 y_scaler: StandardScaler):
        self.model = model
        self.x_scaler = x_scaler
        self.y_scaler = y_scaler

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def _create_sequences(self, data: np.ndarray) -> np.ndarray:
        """
        2D ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ 3D ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜.
        """
        sequences = []
        for i in range(len(data) - 12 + 1):
            sequences.append(data[i : i + 12])
        return np.array(sequences)

    def predict(self, context, model_input: pd.DataFrame) -> np.ndarray:
        """
        ì–´ë–¤ ê¸¸ì´ì˜ ì…ë ¥ ë°ì´í„°ë“  ë°›ì•„, ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ì „ì²´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        :param context: MLflow ì»¨í…ìŠ¤íŠ¸ (ì—¬ê¸°ì„œëŠ” ì‚¬ìš© ì•ˆ í•¨)
        :param model_input: ì˜ˆì¸¡í•  ì›ë³¸ ë°ì´í„° (pandas DataFrame)
        :return: ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ (numpy array)
        """
        # 1. ì…ë ¥ ë°ì´í„°ë¥¼ StandardScalerë¡œ ìŠ¤ì¼€ì¼ë§í•©ë‹ˆë‹¤.
        scaled_input_np = self.x_scaler.transform(model_input)
        
        # 2. ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¥¼ 3D ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        #    ì…ë ¥ ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        if len(scaled_input_np) < 12:
            return np.array([])
            
        sequences_np = self._create_sequences(scaled_input_np)
        sequences_tensor = torch.FloatTensor(sequences_np)

        # 3. ì˜ˆì¸¡ì„ ìœ„í•´ PyTorch DataLoaderë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        pred_dataset = TensorDataset(sequences_tensor)
        pred_loader = DataLoader(pred_dataset, batch_size=64, shuffle=False)
        
        # 4. ëª¨ë¸ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        self.model.eval().to(self.device)
        predictions_scaled = []
        with torch.no_grad():
            for x_batch_tuple in pred_loader:
                x_batch = x_batch_tuple[0].to(self.device)
                y_pred_batch = self.model(x_batch)
                predictions_scaled.append(y_pred_batch.cpu().numpy())
        
        predictions_scaled = np.concatenate(predictions_scaled, axis=0)

        # 5. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›í•©ë‹ˆë‹¤.
        prediction_unscaled = self.y_scaler.inverse_transform(predictions_scaled)
        
        return prediction_unscaled.flatten()


def train_epoch(
    model: nn.Module,
    data_loader: DataLoader,
    criterion: nn.Module,
    optimizer: Optimizer,
    device: torch.device,
    weights_path: Optional[str] = None,
    clip: Optional[float] = None
) -> float:
    """
    ëª¨ë¸ì„ í•œ ì—í­(epoch) ë™ì•ˆ í•™ìŠµì‹œí‚¤ê³  í‰ê·  ì†ì‹¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        model (nn.Module): í•™ìŠµí•  PyTorch ëª¨ë¸.
        data_loader (DataLoader): í•™ìŠµ ë°ì´í„°ë¡œë”.
        criterion (nn.Module): ì†ì‹¤ í•¨ìˆ˜ (ì˜ˆ: nn.MSELoss).
        optimizer (torch.optim.Optimizer): ì˜µí‹°ë§ˆì´ì € (ì˜ˆ: torch.optim.Adam).
        device (torch.device): í•™ìŠµì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu').
        weights_path (Optional[str], optional): ë¶ˆëŸ¬ì˜¬ ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ. ê¸°ë³¸ê°’ì€ None.
        clip (Optional[float], optional): ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘(gradient clipping) ì„ê³„ê°’. 
                                           RNN ê³„ì—´ ëª¨ë¸ì˜ ì•ˆì •ì ì¸ í•™ìŠµì— ë„ì›€ì„ ì¤ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ None.

    Returns:
        Tuple[float, float]: ì—í­ì˜ í‰ê·  ì†ì‹¤(average loss)ê³¼ í‰ê·  ì •í™•ë„(average accuracy).
    """
    # ì €ì¥ëœ ê°€ì¤‘ì¹˜ê°€ ìˆë‹¤ë©´ ë¶ˆëŸ¬ì˜¤ê¸°
    if weights_path:
        try:
            model.load_state_dict(torch.load(weights_path, map_location=device))
            print(f"âœ… '{weights_path}' ì—ì„œ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        except FileNotFoundError:
            print(f"âš ï¸ ê²½ê³ : '{weights_path}' ê²½ë¡œì— íŒŒì¼ì´ ì—†ì–´ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ğŸš¨ ì˜¤ë¥˜: ê°€ì¤‘ì¹˜ ë¡œë”© ì¤‘ ë¬¸ì œ ë°œìƒ - {e}")

    model.train()  # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •

    epoch_loss = 0.0
    total_samples = 0

    # ë°ì´í„°ë¡œë”ë¥¼ ìˆœíšŒí•˜ë©° ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ í•™ìŠµ ì§„í–‰
    for batch_idx, (inputs, targets) in enumerate(data_loader):
        inputs, targets = inputs.to(device), targets.to(device)

       # íšŒê·€ì—ì„œëŠ” targetì˜ shapeì„ ëª¨ë¸ ì¶œë ¥ê³¼ ë§ì¶°ì£¼ì–´ì•¼ í•  ìˆ˜ ìˆìŒ
        # ì˜ˆ: targetì´ [batch_size]ì¼ ê²½ìš°, [batch_size, 1]ë¡œ ë³€ê²½
        if targets.ndim == 1:
            targets = targets.unsqueeze(1)

        # ìˆœì „íŒŒ (Forward pass)
        outputs = model(inputs)
        loss = criterion(outputs, targets)

        # ì—­ì „íŒŒ ë° ìµœì í™” (Backward pass and optimization)
        optimizer.zero_grad()
        loss.backward()

        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (Gradient Clipping)
        if clip:
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
            
        optimizer.step()

        # ì—í­ ì „ì²´ì˜ ì†ì‹¤ ì§‘ê³„
        epoch_loss += loss.item() * inputs.size(0)
        total_samples += targets.size(0)
        
        # ì§„í–‰ ìƒí™© ë¡œê¹…
        if (batch_idx + 1) % 100 == 0:
            print(f"  Batch {batch_idx + 1}/{len(data_loader)} | Batch Loss: {loss.item():.4f}")

    # ì—í­ì˜ í‰ê·  ì†ì‹¤ ê³„ì‚°
    avg_epoch_loss = epoch_loss / total_samples
    
    return avg_epoch_loss


def predict_single(
    model: nn.Module,
    input_sequence: torch.Tensor,
    device: torch.device
) -> float:
    """
    ë‹¨ì¼ ì…ë ¥ ì‹œí€€ìŠ¤ì— ëŒ€í•œ íšŒê·€ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Args:
        model (nn.Module): í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸.
        input_sequence (torch.Tensor): ì˜ˆì¸¡ì— ì‚¬ìš©í•  ë‹¨ì¼ ì…ë ¥ ì‹œí€€ìŠ¤ ë°ì´í„°.
                                       í˜•íƒœ: (sequence_length, input_size)
        device (torch.device): ì—°ì‚°ì„ ìˆ˜í–‰í•  ë””ë°”ì´ìŠ¤.

    Returns:
        float: ëª¨ë¸ì˜ ë‹¨ì¼ ì˜ˆì¸¡ê°’.
    """
    model.eval()
    
    # 1. ì…ë ¥ ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    input_tensor = input_sequence.to(device)
    
    # 2. ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ ë°°ì¹˜ ì°¨ì›(batch dimension) ì¶”ê°€
    # ëª¨ë¸ì€ (batch_size, sequence_length, input_size) í˜•íƒœë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ,
    # (sequence_length, input_size) -> (1, sequence_length, input_size)ë¡œ ë³€ê²½
    input_tensor = input_tensor.unsqueeze(0)

    with torch.no_grad():
        prediction = model(input_tensor)
        
    # 3. ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™ì‹œí‚¤ê³  ìŠ¤ì¹¼ë¼ ê°’ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
    return prediction.cpu().item()


def predict_batch(
    model: nn.Module,
    batch_input: torch.Tensor,
    device: torch.device
) -> np.ndarray:
    """
    ë‹¨ì¼ ì…ë ¥ ë°°ì¹˜ì— ëŒ€í•œ ëª¨ë¸ì˜ íšŒê·€ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ , ê²°ê³¼ë¥¼ NumPy ë°°ì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        model (nn.Module): í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸.
        batch_input (torch.Tensor): ì˜ˆì¸¡í•  ì…ë ¥ ë°ì´í„° ë°°ì¹˜.
        device (torch.device): ì—°ì‚°ì„ ìˆ˜í–‰í•  ë””ë°”ì´ìŠ¤.

    Returns:
        np.ndarray: ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ ë‹´ì€ NumPy ë°°ì—´.
    """
    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •

    with torch.no_grad():  # ê²½ì‚¬ë„ ê³„ì‚° ë¹„í™œì„±í™”
        predictions = model(batch_input.to(device))
    
    # ì˜ˆì¸¡ê°’ì„ CPUë¡œ ì´ë™ì‹œí‚¤ê³  NumPy ë°°ì—´ë¡œ ë³€í™˜
    predictions = predictions.cpu().numpy()

    return predictions


def predict(
    model: nn.Module,
    data_loader: DataLoader,
    device: torch.device
) -> np.ndarray:
    """
    ì „ì²´ ë°ì´í„°ë¡œë”ì— ëŒ€í•´ íšŒê·€ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ , 
    ëª¨ë“  ì˜ˆì¸¡ê°’ì„ í•˜ë‚˜ì˜ NumPy ë°°ì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        model (nn.Module): í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸.
        data_loader (DataLoader): ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ì „ì²´ ë°ì´í„°ì…‹ì˜ ë°ì´í„°ë¡œë”.
                               ì´ ë°ì´í„°ë¡œë”ëŠ” ì…ë ¥ í…ì„œ(X)ë§Œì„ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
        device (torch.device): ì—°ì‚°ì„ ìˆ˜í–‰í•  ë””ë°”ì´ìŠ¤.

    Returns:
        np.ndarray: ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ë“¤ì„ ë‹´ì€ ë‹¨ì¼ NumPy ë°°ì—´.
    """
    model.eval()
    all_predictions = []

    # !!!!!!!!!! ë°ì´í„°ë¡œë”ê°€ ì…ë ¥ í…ì„œ(inputs)ë§Œ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì • í•„ìš”
    for inputs, _ in data_loader:
        # predict_batchëŠ” ë°°ì¹˜ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ np.ndarrayë¡œ ë°˜í™˜
        batch_predictions_np = predict_batch(model, inputs, device)
        all_predictions.append(batch_predictions_np)
    
    # NumPy ë°°ì—´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ í° ë°°ì—´ë¡œ ê²°í•©
    all_predictions = np.concatenate(all_predictions, axis=0)
    
    return all_predictions